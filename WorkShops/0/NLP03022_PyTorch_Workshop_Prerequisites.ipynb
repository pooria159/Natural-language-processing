{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6oqGiIXvrMl"
      },
      "source": [
        "# NLP02031: PyTorch Workshop\n",
        "Having question about the workshop or this notebook? Contact Erfan Moosavi Monazzah (Tel: @ErfanMoosavi2000).\n",
        "<br>This notebook is adapted from CS224n PyTorch Workshop\n",
        "#### Plan\n",
        "This notebook is an itroduction to pytorch and is a prerequisite for the workshop. We cover the following topics in this notebook:\n",
        "1. Tensors\n",
        "2. Vectorization\n",
        "3. Indexing & Slicing\n",
        "4. AutoGrad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gk1UKaNvrMv"
      },
      "source": [
        "## Introduction\n",
        "PyTorch is a deep learning framework, one of the two main frameworks alongside TensorFlow.\n",
        "<br>PyTorch is a popular choice among researchers and practitioners for its ease of use, flexibility, and dynamic computation graph. It allows for seamless use of GPUs, and offers extensive support for common neural network architectures and modules.\n",
        "\n",
        "Some of PyTorch's capabilities include:\n",
        "- Dynamic computation graph\n",
        "- Easy debugging and visualization with tensorboard\n",
        "- Distributed training on multiple GPUs and machines\n",
        "- Support for various neural network architectures and modules, including convolutional and recurrent neural networks, transformers, and more.\n",
        "<br> Let's start by importing PyTorch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "u0ukr7quvrMx"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'torch'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
          ]
        }
      ],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k10ZRdcBwDP3"
      },
      "source": [
        "We are all set to start our tutorial. Let's dive in!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLdSN9ZXvrM0"
      },
      "source": [
        "## Tensors\n",
        "**Tensors** are\n",
        "- PyTorch's most basic building block.\n",
        "- multi-dimensional matrices.\n",
        "\n",
        "for example: A 256x256 image might be represented by a `3x256x256` tensor (First dimension represents color channels)\n",
        "<br><font color='yellow'>Quiz: How can we represent a sentence using tensors? ðŸ¤”</font>\n",
        "\n",
        "**Answer:**<br>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXD7cTLTh4oF",
        "outputId": "c09662c6-31d4-49f8-fd16-d596fdcc029c"
      },
      "outputs": [],
      "source": [
        "list_of_lists = [\n",
        "  [1, 2, 3],\n",
        "  [4, 5, 6],\n",
        "]\n",
        "print(list_of_lists)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-VLgHhj0n3LM",
        "outputId": "a04364d9-0eaf-45cb-98c5-c223e3457203"
      },
      "outputs": [],
      "source": [
        "# Initializing a tensor\n",
        "data = torch.tensor([list_of_lists])\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1i7vrR1_oO4I"
      },
      "source": [
        "Each tensor has a **data type**, something like:\n",
        "- `torch.float32`\n",
        "- `torch.int`\n",
        "<br>You can specify the data type explicitly when you create the tensor:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7nMdqgMoLOa",
        "outputId": "684dfbff-629d-4624-b3c6-42fb8f4eb8c5"
      },
      "outputs": [],
      "source": [
        "# Notice the dots after the numbers, which specify that they're floats\n",
        "data = torch.tensor([\n",
        "                     [0, 1],\n",
        "                     [2, 3],\n",
        "                     [4, 5]\n",
        "                    ], dtype=torch.float32)\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aiGCmTsrpkP-"
      },
      "source": [
        "There are a number of utility functions to create tensors in pytorch:\n",
        "- **torch**.zeros(): creates a tensor filled with zeros.\n",
        "- **torch**.ones(): creates a tensor filled with ones.\n",
        "- **torch**.rand(): creates a tensor filled with random values from this range [0, 1).\n",
        "- **torch**.full(): creates a tensor filled with a scalar value.\n",
        "- **torch**.eye(): creates a square tensor with ones on the diagonal and zeros elsewhere.\n",
        "- **torch**.arange(): creates a 1D tensor with evenly spaced values in a given range, space determined by step.\n",
        "- **torch**.linspace(): creates a 1D tensor with evenly spaced values between a start and end value, space determined by number of values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0tn-N-z_qYj0",
        "outputId": "6c8347cf-0ea0-4215-c5d5-86473aa11dd0"
      },
      "outputs": [],
      "source": [
        "zeros = torch.zeros(2, 5)  # shape\n",
        "ones = torch.ones(3, 4) # shape\n",
        "randoms = torch.rand(2, 3) # shape\n",
        "full = torch.full((3,4), 56.7) # shape, fill_value\n",
        "I = torch.eye(3) # diagonal_size\n",
        "arange = torch.arange(0, 10, 2) # start, stop, step\n",
        "linspace = torch.linspace(0, 10, 20) # start, stop, number_of_values\n",
        "empty = torch.empty(2,2) # shape: faster than zeros() or ones() because it does not init the memory it alloc\n",
        "\n",
        "print(\"zeros = torch.zeros(2, 5)\\n\", zeros, \"\\n\\n\",\n",
        "      \"ones = torch.ones(3, 4)\\n\", ones, \"\\n\\n\",\n",
        "      \"randoms = torch.rand(2, 3)\\n\", randoms, \"\\n\\n\",\n",
        "      \"full = torch.full((3, 4), 56.7)\\n\", full, \"\\n\\n\",\n",
        "      \"I = torch.eye(3)\\n\", I, \"\\n\\n\",\n",
        "      \"arange = torch.arange(0, 10, 2)\\n\", arange, \"\\n\\n\",\n",
        "      \"linspace = torch.linspace(0, 10, 20)\\n\", linspace, \"\\n\\n\",\n",
        "      \"empty = torch.empty(2,2)\\n\", empty)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBNWAJBFJcuS"
      },
      "source": [
        "<font color=\"yellow\">Quiz: Under each comment write the suitable script to create the said tensor</font>\n",
        "<br>$A=\\begin{bmatrix} 1 & 2.2 & 9.6 \\\\ 4 & -7.2 & 6.3 \\end{bmatrix}$\n",
        "<br>$B=\\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix}$ (Initialize with 3 different ways?)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xwTkKjjJcuT",
        "outputId": "4dba7c1b-07cb-4641-f05c-ed7488498ab8"
      },
      "outputs": [],
      "source": [
        "# A\n",
        "A = Your Code Goes Here\n",
        "print(A)\n",
        "\n",
        "# B\n",
        "B1 = Your Code Goes Here\n",
        "B2 = Your Code Goes Here\n",
        "B3 = Your Code Goes Here\n",
        "print(B1, '\\n', B2, '\\n', B3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFcjwshvi3pC",
        "outputId": "5e6a3e82-54ec-4d94-e102-dec816771409"
      },
      "outputs": [],
      "source": [
        "# Create two random tensors\n",
        "A = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
        "B = A.clone() # Modifiying the clone does not affect the original tensor\n",
        "\n",
        "A_shape = A.shape # A Size object containing tensor dimentions' sizes\n",
        "\n",
        "# Addition\n",
        "C = A + B\n",
        "C_torch = torch.add(A, B)\n",
        "\n",
        "# Subtraction\n",
        "D = A - B\n",
        "D_torch = torch.sub(A, B)\n",
        "\n",
        "# Multiplication (element-wise)\n",
        "E = A * B\n",
        "E_torch = torch.mul(A, B)\n",
        "\n",
        "# Division (element-wise)\n",
        "F = A / B\n",
        "F_torch = torch.div(A, B)\n",
        "\n",
        "# Transpose\n",
        "G = A.T\n",
        "G_torch = torch.transpose(A, 0, 1)\n",
        "\n",
        "# Matrix multiplication\n",
        "H = A @ B.T\n",
        "H_torch = torch.matmul(A, B.T)\n",
        "\n",
        "# Print results\n",
        "print(\"A: \\n\", A)\n",
        "print(\"B: \\n\", B)\n",
        "print(\"A.shape: \\n\", A_shape)\n",
        "print(\"A + B: \\n\", C)\n",
        "print(\"torch.add(A, B): \\n\", C_torch)\n",
        "print(\"A - B: \\n\", D)\n",
        "print(\"torch.sub(A, B): \\n\", D_torch)\n",
        "print(\"A * B: \\n\", E)\n",
        "print(\"torch.mul(A, B): \\n\", E_torch)\n",
        "print(\"A / B: \\n\", F)\n",
        "print(\"torch.div(A, B): \\n\", F_torch)\n",
        "print(\"Transpose of A: \\n\", G)\n",
        "print(\"torch.transpose(A, 0, 1): \\n\", G_torch)\n",
        "print(\"A @ B.T: \\n\", H)\n",
        "print(\"torch.matmul(A, B.T): \\n\", H_torch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Pz2km8FJcuT"
      },
      "source": [
        "<font color=\"yellow\">Quiz: Considering tensor A and B, implement the following formula: </font>\n",
        "<br>$((A+B)(A-B)^T)/I$\n",
        "<br>$A=\\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix}$\n",
        "<br>$B=\\begin{bmatrix} 2 & 3 & 4 \\\\ 5 & 6 & 7 \\end{bmatrix}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGpAUQYJJcuT",
        "outputId": "5e07c729-3019-411d-d899-058c10e34e3f"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Create two tensors A and B\n",
        "A = Your Code Goes Here\n",
        "B = Your Code Goes Here\n",
        "print(A)\n",
        "print(B)\n",
        "\n",
        "# Perform the calculation\n",
        "result = Your Code Goes Here\n",
        "\n",
        "# Print the result\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wKqP85rrF-P"
      },
      "source": [
        "**Reshaping** tensors can be used to make batch operations easier (more on that later), but be careful that the data is reshaped in the order you expect:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HmUcqHYUrMu1",
        "outputId": "ecbbb09d-d4a2-4756-8d44-7a848dd1d47c"
      },
      "outputs": [],
      "source": [
        "rr = torch.arange(1, 16)\n",
        "print(\"The shape is currently\", rr.shape)\n",
        "print(\"The contents are currently\", rr)\n",
        "print()\n",
        "rr2 = rr.view(5, 3) # view is not a clone, it uses the same shared data\n",
        "print(\"After reshaping, the shape is currently\", rr2.shape)\n",
        "print(\"The contents are currently\", rr2)\n",
        "print()\n",
        "print('Changing the first value of rr will change the corresponding value in rr2')\n",
        "rr[0] = 100\n",
        "print(rr)\n",
        "print(rr2)\n",
        "\n",
        "# what about reshape?\n",
        "# read: https://stackoverflow.com/questions/49643225/whats-the-difference-between-reshape-and-view-in-pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaykBuhoou3M"
      },
      "source": [
        "Finally, you can also inter-convert tensors with **NumPy arrays**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ppYiPnlko1Ci",
        "outputId": "a99c3ca7-3e5a-4e70-f9c4-bc015be3531b"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# numpy.ndarray --> torch.Tensor: [feed ndarray to torch.tensor]\n",
        "arr = np.array([[1, 0, 5]])\n",
        "data = torch.tensor(arr)\n",
        "print(\"This is a torch.tensor\", data)\n",
        "\n",
        "# torch.Tensor --> numpy.ndarray: [use .numpy() on tensor]\n",
        "new_arr = data.numpy()\n",
        "print(\"This is a np.ndarray\", new_arr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyv1l431q9yA"
      },
      "source": [
        "## Vectorization\n",
        "One of the reasons why we use **tensors** is *vectorized operations*: operations that be conducted in parallel over a particular dimension of a tensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kas2MEFDsJWk",
        "outputId": "607fc5a1-e824-4413-a293-deaa7b815902"
      },
      "outputs": [],
      "source": [
        "data = torch.arange(1, 36, dtype=torch.float32).reshape(5, 7)\n",
        "print(\"Data is:\", data)\n",
        "\n",
        "# We can perform operations like *sum* over each row...\n",
        "print(\"Row sum:\")\n",
        "print(data.sum(dim=0))\n",
        "\n",
        "# or over each column.\n",
        "print(\"Col sum:\")\n",
        "print(data.sum(dim=1))\n",
        "\n",
        "# Other operations are available:\n",
        "print(\"Col std:\")\n",
        "print(data.std(dim=1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGRreztsJcuV"
      },
      "source": [
        "**Without specifying dimentions, it just sum all the values**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPRy-xtuk2tK",
        "outputId": "c2f25d93-e167-42d7-8ea5-ae316e13ae40"
      },
      "outputs": [],
      "source": [
        "data.sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJ8MjWEMxOVk"
      },
      "source": [
        "\n",
        "\n",
        "<font color=\"yellow\">Quiz: Write code that creates a `torch.tensor` with the following contents:\n",
        "$\\begin{bmatrix} 1 & 2.2 & 9.6 \\\\ 4 & -7.2 & 6.3 \\end{bmatrix}$\n",
        "\n",
        "Normalize the values in the tensor using Z-Score normalization. (Help: https://en.wikipedia.org/wiki/Standard_score)</font>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BK0YInGkn3Xy"
      },
      "outputs": [],
      "source": [
        "tens = Your Code Goes Here\n",
        "\n",
        "tens_normalized = Your Code Goes Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7BMktFFAkRA"
      },
      "source": [
        "## Indexing & Slicing\n",
        "\n",
        "You can access arbitrary elements of a tensor using the `[]` operator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvkHJ2pYmgMD",
        "outputId": "b32e0403-7a29-4584-f148-7372d8d6d412"
      },
      "outputs": [],
      "source": [
        "matr = torch.arange(1, 16).view(5, 3)\n",
        "print(matr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXfgmuplmpmg"
      },
      "outputs": [],
      "source": [
        "matr[0] # first row"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rw_qQ9ponJV2",
        "outputId": "11c82045-80f1-44b1-b9fb-e4f80e029911"
      },
      "outputs": [],
      "source": [
        "matr[0, :] # first row"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yw2H2WOcmuyz",
        "outputId": "466d6ccf-4299-426f-85f7-994ec7d808c0"
      },
      "outputs": [],
      "source": [
        "matr[:, 0] # first col"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9f0q9ZFmysw",
        "outputId": "c6d835af-39fb-49f6-a9aa-ee26af182a9f"
      },
      "outputs": [],
      "source": [
        "matr[0:3] # 3 by 3 from top left"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ucc2UqRVm08h",
        "outputId": "a43e4a92-b7c5-449f-aa22-773eeaff8e71"
      },
      "outputs": [],
      "source": [
        "matr[:, 0:2] # first two cols"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEkhpZiRm24y",
        "outputId": "1c260e86-3ad7-4465-e7b8-a366a4bf7bb3"
      },
      "outputs": [],
      "source": [
        "matr[0:3, 0:2] # frist three rows of the first two cols"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHtpOdS5JcuX"
      },
      "source": [
        "<font color=\"yellow\">Quiz: It look likes they are doing the same thing? Can you give an example that matr[i][j] wouldn't be the same as matr[i,j]?</font>\n",
        "<br>hint: You may consider i and/or j as slices not indices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kN7oaYPdm-zC",
        "outputId": "4ff017fc-e252-4c09-f66a-eb4f3f67c9db"
      },
      "outputs": [],
      "source": [
        "print(matr[0][2])\n",
        "print(matr[0,2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EdYiUYpx_bVu",
        "outputId": "88fd261b-609d-4e40-8e67-20b0188421a7"
      },
      "outputs": [],
      "source": [
        "print(Your Code Goes Here)\n",
        "print(Your Code Goes Here)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoOkN7OiJcuX"
      },
      "source": [
        "Accessing python scalar value in a tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BM-ZujN2IGaQ",
        "outputId": "9a071c39-6c0a-426e-8d82-762dec4854a8"
      },
      "outputs": [],
      "source": [
        "matr[0, 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6NwxK7d_Ycgs",
        "outputId": "863e3732-6c55-4e69-aa20-2365673f941b"
      },
      "outputs": [],
      "source": [
        "matr[0, 0].item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Re8xiL37eAja"
      },
      "source": [
        "## Autograd\n",
        "Pytorch is well-known for its automatic differentiation feature. We can call the `backward()` method to ask `PyTorch` to calculate the gradients, which are then stored in the `grad` attribute."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oEvBJHWfn8H",
        "outputId": "5ac8edf0-bf9a-4ba7-99c0-3c56a5f99d24"
      },
      "outputs": [],
      "source": [
        "# Create an example tensor\n",
        "# requires_grad parameter tells PyTorch to store gradients\n",
        "x = torch.tensor([2.], requires_grad=True)\n",
        "\n",
        "# Print the gradient if it is calculated\n",
        "# Currently None since x is a scalar\n",
        "print(x.grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTJazZXkgthP",
        "outputId": "cac4828d-841d-427e-b1ba-08819162453e"
      },
      "outputs": [],
      "source": [
        "# Calculating the gradient of y with respect to x\n",
        "y = x * x * 3 # 3x^2\n",
        "y.backward()\n",
        "print(x.grad) # d(y)/d(x) = d(3x^2)/d(x) = 6x = 12"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Hqc2oM3iV6a"
      },
      "source": [
        "Let's run backprop from a different tensor again to see what happens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K--Az0Xiic_z",
        "outputId": "b61760e7-8f2a-4a37-c752-2f88981cad73"
      },
      "outputs": [],
      "source": [
        "z = x * x * 3 # 3x^2\n",
        "z.backward()\n",
        "print(x.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhjPkiE6i7ja"
      },
      "source": [
        "We can see that the `x.grad` is updated to be the sum of the gradients calculated so far. When we run backprop in a neural network, we sum up all the gradients for a particular neuron before making an update. This is exactly what is happening here! This is also the reason why we need to run `zero_grad()` in every training iteration (more on this later). Otherwise our gradients would keep building up from one training iteration to the other, which would cause our updates to be wrong."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fU6ceW-wJcuk",
        "outputId": "3874e312-3ebb-4620-d9a3-0a179a6dacbe"
      },
      "outputs": [],
      "source": [
        "# let's have a look at a bit more sophisticated example:\n",
        "# clearing cumulative grads\n",
        "print(x.grad)\n",
        "x.grad.zero_()\n",
        "print(x.grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d68tSXeiJcuk",
        "outputId": "bb0137b2-6524-462b-a72b-db150e99e97d"
      },
      "outputs": [],
      "source": [
        "y = torch.tensor(3., requires_grad=True)\n",
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7F2cdxuJcuk",
        "outputId": "0de84893-b0ce-4f20-b28f-86f6b9b8e61c"
      },
      "outputs": [],
      "source": [
        "f = y * x * 10\n",
        "f.backward()\n",
        "print(x)\n",
        "print(x.grad)\n",
        "print()\n",
        "print(y)\n",
        "print(y.grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yL3Q8y-ZJcuk",
        "outputId": "c3883ff5-e9a5-451b-e414-8aaf1831e118"
      },
      "outputs": [],
      "source": [
        "x.grad.zero_()\n",
        "f = 3 * x\n",
        "g = 10 * f\n",
        "g.backward()\n",
        "\n",
        "print(x)\n",
        "print(x.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tcrK2mtJcul"
      },
      "source": [
        "To read more about auto grad:\n",
        "1) https://pytorch.org/blog/computational-graphs-constructed-in-pytorch/\n",
        "2) https://pytorch.org/tutorials/beginner/introyt/autogradyt_tutorial.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URssDpggJcul"
      },
      "source": [
        "<font color=\"yellow\">Quiz: Draw the following functions with matplotlib.</font>\n",
        "<br>$x \\epsilon [-3, 3]$\n",
        "1. $f(x) = \\frac{1}{\\sqrt{2\\pi} e^{\\frac{x+1}{2}}}$\n",
        "2. $\\frac{\\partial }{\\partial x} f(x)$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "SO3FwZGGJcul",
        "outputId": "4c3d5250-7f44-4e49-8f22-6c1af5ab9bf8"
      },
      "outputs": [],
      "source": [
        "Your Code Goes Here"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
