{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Q2"
      ],
      "metadata": {
        "id": "IyfC-UCj19LU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "*   In this question we will Build a bigram HMM tagger.\n",
        "*   First we split the part-of-speech-tagged corpus into a training set and test set.\n",
        "*   From the labeled training set, we train the transition and observation probabilities of the HMM tagger directly on the hand-tagged data.\n",
        "*   Then implement the Viterbi algorithm so we can decode a test sentence.\n",
        "*   Run the algorithm on the test set. Report its error rate and\n",
        "compare its performance to the most frequent tag baseline.\n",
        "*   Build a confusion matrix and investigate the most frequent errors.\n"
      ],
      "metadata": {
        "id": "iV_jVxtx2Adw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('universal_tagset')"
      ],
      "metadata": {
        "id": "bUE94vu28kjr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import nltk\n",
        "nltk.download('brown')\n",
        "from nltk.corpus import brown"
      ],
      "metadata": {
        "id": "1i6zb0gc6gJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "BUnjwJLukB3r"
      },
      "outputs": [],
      "source": [
        "def collect_probabilities(_samples):\n",
        "    \"\"\"\n",
        "    Collects various probabilities and frequencies required for Hidden Markov Model (HMM) training based on given samples.\n",
        "\n",
        "    Args:\n",
        "    _samples (list): A list of tuples containing (word, tag) pairs representing training data.\n",
        "\n",
        "    Returns:\n",
        "    tuple: A tuple containing four dictionaries:\n",
        "           - tag_freq: A dictionary storing the frequency of each tag type.\n",
        "           - word_per_tag_freq: A nested dictionary storing the frequency of each word under each tag.\n",
        "           - bigram: A nested dictionary representing the bigram probabilities between tag types.\n",
        "           - pi: A dictionary storing the initial probability distributions for tag types.\n",
        "\n",
        "    Comments:\n",
        "    - tag_freq: Stores the frequency of each tag type observed in the training data.\n",
        "    - word_per_tag_freq: Stores the frequency of each word observed under each tag in the training data.\n",
        "      - Each word under a specific tag is initialized with a count of 2 to provide initial smoothing.\n",
        "    - bigram: Represents the transition probabilities between tag types based on bigram counts.\n",
        "    - pi: Stores the initial probability distributions for tag types based on the second tag in each sample.\n",
        "    \"\"\"\n",
        "    tag_freq = {}\n",
        "    word_per_tag_freq = {}\n",
        "    bigram = {}\n",
        "    pi = {}\n",
        "    samples_len = len(_samples)\n",
        "\n",
        "    # Iterate through samples to collect tag frequencies, word frequencies under each tag, and initial probabilities\n",
        "    for i in range(samples_len):\n",
        "        sample = _samples[i]\n",
        "        has_next = i + 1 < samples_len\n",
        "\n",
        "        # Collect data for initial probabilities (pi)\n",
        "        if has_next:\n",
        "            if _samples[i + 1][1] not in pi:\n",
        "                pi.update({_samples[i + 1][1]: 2})\n",
        "            else:\n",
        "               # Your code here\n",
        "\n",
        "        # Count tag frequencies (tag_freq) and word frequencies under each tag (word_per_tag_freq)\n",
        "        if sample[1] not in tag_freq:\n",
        "            tag_freq.update({sample[1]: 2})\n",
        "            word_per_tag_freq.update({sample[1]: {sample[0]: 2}})\n",
        "        else:\n",
        "           # Your code here\n",
        "\n",
        "\n",
        "    # Initialize the bigram matrix with default counts\n",
        "    for tag_0 in tag_freq:\n",
        "        bigram.update({tag_0: {}})\n",
        "        for tag_1 in tag_freq:\n",
        "            bigram[tag_0].update({tag_1: 1})\n",
        "\n",
        "    # Count bigram occurrences\n",
        "    for i in range(samples_len):\n",
        "      # Your code here\n",
        "\n",
        "    # Return collected probabilities and frequencies\n",
        "    return tag_freq, word_per_tag_freq, bigram, pi\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_confusion_matrix(predictions, hidden_state, confusion_matrices , is_correct):\n",
        "    \"\"\"\n",
        "    Updates the confusion matrices based on the predictions and hidden states,\n",
        "    calculates the number of correct predictions, and identifies tags with\n",
        "    the most false positives and false negatives.\n",
        "\n",
        "    Args:\n",
        "    predictions (list): Predicted sequence of tags for the given input data.\n",
        "    hidden_state (list): Actual sequence of tags for the given input data.\n",
        "    confusion_matrices (dict): A dictionary containing confusion matrices for each tag.\n",
        "    is_correct (int): Counter for the number of correct predictions.\n",
        "\n",
        "    Returns:\n",
        "    int: Updated count of correct predictions after processing the input data.\n",
        "\n",
        "    Comments:\n",
        "    - As the Viterbi algorithm processes each sentence in the test data,\n",
        "      it compares the predicted tags against the actual tags for each word in the sentence.\n",
        "        - For each word in the sentence, if the predicted tag matches the actual tag (a true positive),\n",
        "          the corresponding count in the confusion matrix for that tag is incremented.\n",
        "        - If the predicted tag does not match the actual tag:\n",
        "            - If the predicted tag is incorrect (a false positive), the FP count for\n",
        "              the predicted tag is incremented, and the TN counts for other tags are incremented.\n",
        "            - If the actual tag is not predicted (a false negative), the FN count for the actual\n",
        "              tag is incremented, and the TN counts for other tags are incremented.\n",
        "    - After updating the confusion matrices, the function identifies tags with\n",
        "      the highest counts of false positives and false negatives.\n",
        "    \"\"\"\n",
        "    # fill in the confusion matrix\n",
        "    for i in range(len(predictions)):\n",
        "        if predictions[i] == hidden_state[i]:\n",
        "            # prediction is correct\n",
        "            is_correct += 1\n",
        "            for j in confusion_matrices:\n",
        "                # Your code here\n",
        "\n",
        "        else:\n",
        "            # prediction is incorrect\n",
        "            for j in confusion_matrices:\n",
        "                # Your code here\n",
        "\n",
        "\n",
        "    # rank\n",
        "    highest_FP_tag = []\n",
        "    highest_FP = -1\n",
        "    highest_FN_tag = []\n",
        "    highest_FN = -1\n",
        "\n",
        "    for tag in confusion_matrices:\n",
        "\n",
        "        print(f'{tag}: {confusion_matrices[tag]}')\n",
        "\n",
        "        if confusion_matrices[tag]['FP'] > highest_FP:\n",
        "            highest_FP = confusion_matrices[tag]['FP']\n",
        "            highest_FP_tag = tag\n",
        "\n",
        "        if confusion_matrices[tag]['FN'] > highest_FN:\n",
        "            highest_FN = confusion_matrices[tag]['FN']\n",
        "            highest_FN_tag = tag\n",
        "\n",
        "    print(f'\\nTag with the most false positives is: {highest_FP_tag} with {highest_FP} counts.')\n",
        "    print(f'Tag with the most false negative is:  {highest_FN_tag} with {highest_FN} counts.')\n",
        "\n",
        "    return is_correct\n"
      ],
      "metadata": {
        "id": "WYyGWqdjNEfp"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def viterbi(_samples, _tag_freq, _word_per_tag_freq, _bigram, _init_dist):\n",
        "    \"\"\"\n",
        "    Performs the Viterbi algorithm for part-of-speech tagging on a given test set of sentences.\n",
        "\n",
        "    Args:\n",
        "    _samples (list): A list of tuples containing (word, tag) pairs representing the test data.\n",
        "    _tag_freq (dict): A dictionary storing the frequency of each tag type observed in the training data.\n",
        "    _word_per_tag_freq (dict): A nested dictionary storing the frequency of each word observed under each tag in the training data.\n",
        "    _bigram (dict): A nested dictionary representing the bigram probabilities between tag types.\n",
        "    _init_dist (dict): A dictionary storing the initial probability distributions for tag types.\n",
        "\n",
        "    Comments:\n",
        "    - This function implements the Viterbi algorithm for part-of-speech tagging.\n",
        "    - It processes each sentence in the test set, using '.' as an indicator that a sentence is over.\n",
        "    - The function populates a confusion matrix to track the performance of the model.\n",
        "    - The Viterbi algorithm is used to find the most likely sequence of tags for each sentence.\n",
        "    - The algorithm iterates through each word in the sentence, calculating the most likely tag sequence based on probabilities and transition probabilities.\n",
        "    - The model's predictions are compared against the actual tags to evaluate accuracy and populate the confusion matrix.\n",
        "    - The function prints detailed information about predictions, including the sentence, hidden states, and predicted tags.\n",
        "    - It also identifies tags with the most false positives and false negatives based on the confusion matrix.\n",
        "    - Finally, the function prints the overall accuracy of the model on the test set.\n",
        "    \"\"\"\n",
        "    test_size = len(_samples)\n",
        "    current_index = 0\n",
        "    is_correct = 0\n",
        "\n",
        "    # populate the confusion matrix\n",
        "    confusion_matrices = {}\n",
        "    for tag in _tag_freq:\n",
        "        confusion_matrices.update({tag: {'TP': 0, 'FP': 0, 'TN': 0, 'FN': 0}})\n",
        "\n",
        "    while current_index < test_size:\n",
        "        # find the boundry of a sentence\n",
        "        sentence = []\n",
        "        hidden_state = []\n",
        "        last_token = ''\n",
        "        while last_token != '.' and current_index < test_size:\n",
        "            sentence.append(''' Your code here ''')\n",
        "            hidden_state.append(''' Your code here ''')\n",
        "            last_token = sentence[-1]\n",
        "            current_index += 1\n",
        "\n",
        "        # initialization step\n",
        "        path_probability = {}\n",
        "        backpointer = {}\n",
        "        for tag in init_dist:\n",
        "            path_probability.update({tag: []})\n",
        "            backpointer.update({tag: [0]})\n",
        "\n",
        "            # initial distribution of this tag\n",
        "            pi_tag = init_dist[tag]\n",
        "\n",
        "            # b_word is the probability of the word being generated by this tag\n",
        "            if sentence[0] in _word_per_tag_freq[tag]:\n",
        "                b_word = # Your code here\n",
        "            else:\n",
        "                b_word = 2.2250738585072014e-100\n",
        "            path_probability[tag].append(''' Your code here ''')\n",
        "\n",
        "        # recursion step\n",
        "        T = len(sentence)\n",
        "        for i in range(1, T):\n",
        "            for tag in init_dist:\n",
        "\n",
        "                if sentence[i] in _word_per_tag_freq[tag]:\n",
        "                    b_word = # Your code here\n",
        "                else:\n",
        "                    b_word = 2.2250738585072014e-100\n",
        "\n",
        "                # search the maximum value of\n",
        "                # viterbi[s', t-1] * a(s|s') *b_s(o_t)\n",
        "                best_trans_prob = -2.2250738585072014e+308\n",
        "                best_trans_tag = ''\n",
        "                for prev_tag in init_dist:\n",
        "                    if prev_tag in _bigram and tag in _bigram[prev_tag]:\n",
        "                        transitional_prob = # Your code here\n",
        "                    else:\n",
        "                        transitional_prob = 2.2250738585072014e-100\n",
        "\n",
        "                    prob = path_probability[prev_tag][i - 1] + math.log(transitional_prob * b_word, 10)\n",
        "\n",
        "                    if prob > best_trans_prob:\n",
        "                        best_trans_prob = prob\n",
        "                        best_trans_tag = prev_tag\n",
        "\n",
        "                path_probability[tag].append(best_trans_prob)\n",
        "                backpointer[tag].append(best_trans_tag)\n",
        "\n",
        "        # termination step\n",
        "        best_path_prob = -2.2250738585072014e+308\n",
        "        best_path_pointer = None\n",
        "        for tag in init_dist:\n",
        "            # Your code here\n",
        "\n",
        "        predictions = [best_path_pointer]\n",
        "\n",
        "        # make the predictions path\n",
        "        for i in reversed(range(1, T)):\n",
        "            # Your code here\n",
        "\n",
        "        predictions.reverse()\n",
        "\n",
        "        print(f'sentence:       {sentence}')\n",
        "        print(f'hidden s:       {hidden_state}')\n",
        "        print(f'predictions:    {predictions}')\n",
        "\n",
        "        is_correct = create_confusion_matrix(predictions, hidden_state, confusion_matrices, is_correct)\n",
        "\n",
        "    print(f'\\nmodel got {is_correct} samples correct out of {test_size}')\n",
        "    print(f'accuracy: {is_correct / test_size}')\n"
      ],
      "metadata": {
        "id": "eQmtEDrd6gvM"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CORPUS = brown.tagged_words(categories='news', tagset='universal')\n",
        "CORPUS_SIZE = len(brown.tagged_words(categories='news'))\n",
        "\n",
        "CUT_OFF = math.floor(CORPUS_SIZE * 0.75)\n",
        "\n",
        "# section off training and testing lists from corpus\n",
        "training_list = CORPUS[:CUT_OFF]\n",
        "testing_list = CORPUS[CUT_OFF:]\n",
        "\n",
        "tag_frequency, word_per_tag_frequency, tag_bigram, init_dist = collect_probabilities(training_list)\n",
        "viterbi(testing_list, tag_frequency, word_per_tag_frequency, tag_bigram, init_dist)\n"
      ],
      "metadata": {
        "id": "Zz2B0nq37tuX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}