{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgQCj6-IRDLF"
      },
      "source": [
        "1.Submit a Google Colab notebook containing your completed code and experimentation results.\n",
        "\n",
        "2.Include comments and explanations in your code to help understand the implemented logic.\n",
        "\n",
        "**Additional Notes:**\n",
        "*   Ensure that the notebook runs successfully in Google Colab.\n",
        "*   Document any issues encountered during experimentation and how you addressed them.\n",
        "\n",
        "**Grading:**\n",
        "*   Each task will be graded out of the specified points.\n",
        "*   Points will be awarded for correctness, clarity of code, thorough experimentation, and insightful analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oCb3cVwzrfbb"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')\n",
        "\n",
        "SOURCE_DIR = 'Q3_data.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HtCgCrUVtU_J"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "from gensim.models import Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZqFaiJ-rRDLJ"
      },
      "outputs": [],
      "source": [
        "def delete_hashtag_usernames(text):\n",
        "  try:\n",
        "    result = []\n",
        "    for word in text.split():\n",
        "      if word[0] not in ['@', '#']:\n",
        "        result.append(word)\n",
        "    return ' '.join(result)\n",
        "  except:\n",
        "    return ''\n",
        "\n",
        "def delete_url(text):\n",
        "  text = re.sub(r'http\\S+', '', text)\n",
        "  return text\n",
        "\n",
        "def delete_ex(text):\n",
        "  text = re.sub(r'\\u200c', '', text)\n",
        "  return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cz4cWWgA4xBF"
      },
      "source": [
        "# 0. Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "t0FjQ9_ZMkve",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77b37a6d-fde3-41f1-ca99-edc2be5742d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting json-lines\n",
            "  Downloading json_lines-0.5.0-py2.py3-none-any.whl (6.8 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from json-lines) (1.16.0)\n",
            "Installing collected packages: json-lines\n",
            "Successfully installed json-lines-0.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install json-lines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Uf7K92-olSfe"
      },
      "outputs": [],
      "source": [
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bG5awXKo40HS",
        "outputId": "742a9d2d-cb5d-44ec-e2cc-77ebd523fca9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Ø§ÛŒØ±Ø§Ù†ÛŒØ§Ù† Ù…Ø§Ù„Ø²ÛŒ Ù‡Ù…Ú¯Ø§Ù… Ø¨Ø§ Ø§ÛŒØ±Ø§Ù†ÛŒØ§Ù† Ø¯Ø± Ø§Ù‚ØµÛŒ Ù†Ù‚Ø§Ø· Ø¯Ù†ÛŒØ§ Ù‡Ù…Ø±Ø§Ù‡ Ùˆ Ù‡Ù…Ø¯Ù„ Ø¨Ø§ Ù‡Ù…ÙˆØ·Ù†Ø§Ù†Ù…Ø§Ù† Ø¯Ø± Ø¯Ø§Ø®Ù„ Ø§ÛŒØ±Ø§Ù† Ù‡Ø³ØªÙ†Ø¯\n",
            "2. Ø§Ù„Ø§Ù† Ø­Ø§Ù„ Ø®Ø§Ù…Ù†Ù‡ Ø§ÛŒ Ø§ÛŒÙ†Ø·ÙˆØ±ÛŒÙ‡\n",
            "3. Ú†Ùˆ Ø¢Ù…Ø¯ Ø®Ø±ÙˆØ´Ø§Ù† Ø¨Ù‡ ØªÙ†Ú¯ Ø§Ù†Ø¯Ø±Ø´ Ø¨Ø¬Ù†Ø¨ÛŒØ¯ Ùˆ Ø¨Ø±Ø¯Ø§Ø´Øª Ø®ÙˆØ¯ Ø§Ø² Ø³Ø±Ø´ Ø±Ù‡Ø§ Ø´Ø¯ Ø² Ø¨Ù†Ø¯ Ø²Ø±Ù‡ Ù…ÙˆÛŒ Ø§ÙˆÛŒ Ø¯Ø±ÙØ´Ø§Ù† Ú†Ùˆ Ø®ÙˆØ±Ø´ÛŒØ¯ Ø´Ø¯ Ø±ÙˆÛŒ Ø§ÙˆÛŒ Ø¨Ø¯Ø§Ù†Ø³Øª Ø³Ù‡Ø±Ø§Ø¨ Ú©Ø§Ùˆ Ø¯Ø®ØªØ±Ø³Øª Ø³Ø± Ùˆ Ù…ÙˆÛŒ Ø§Ùˆ Ø§Ø²Ø¯Ø± Ø§ÙØ³Ø±Ø³Øª Ø´Ú¯ÙØª Ø¢Ù…Ø¯Ø´ Ú¯ÙØª Ø§Ø² Ø§ÛŒØ±Ø§Ù† Ø³Ù¾Ø§Ù‡ Ú†Ù†ÛŒÙ† Ø¯Ø®ØªØ± Ø¢ÛŒØ¯ Ø¨Ù‡ Ø¢ÙˆØ±Ø¯Ú¯Ø§Ù‡\n",
            "4. Ø¨Ø±Ø§ÛŒ Ø§ÛŒØ±Ø§Ù† Ø¨Ø±Ø§ÛŒ Ø§ÛŒØ±Ø§Ù†ğŸ¤\n",
            "5. Ø¨Ø±Ø§ÛŒ 38\n"
          ]
        }
      ],
      "source": [
        "# Reading the CSV file\n",
        "df = pd.read_csv(SOURCE_DIR)\n",
        "\n",
        "# Deleting usernames, hashtags, and web addresses from the texts and saving them in a list\n",
        "texts = list(set(df['Text'].map(delete_hashtag_usernames).map(delete_url).map(delete_ex)))\n",
        "\n",
        "# Removing periods and semicolons from the texts\n",
        "texts = [re.sub(r\"[{}]\".format(string.punctuation + 'ØŸØŒØ›!'), \" \", text).strip() for text in texts]\n",
        "\n",
        "# Removing duplicate texts\n",
        "texts = list(set(texts))\n",
        "\n",
        "# Displaying the first 100 tweets along with line numbers in a column format\n",
        "for i, text in enumerate(texts[:5], start=1):\n",
        "    print(f\"{i}. {text}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sS3rx4CRDLK"
      },
      "source": [
        "# 1. Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axWhDtxIRDLK"
      },
      "source": [
        "## Cosine Similarity\n",
        "\n",
        "To measure the similarity between two words, you need a way to measure the degree of similarity between two embedding vectors for the two words. Given two vectors $u$ and $v$, cosine similarity is defined as follows:\n",
        "\n",
        "$$\\text{CosineSimilarity(u, v)} = \\frac {u \\cdot v} {||u||_2 ||v||_2} = cos(\\theta)Â \\tag{1}$$\n",
        "\n",
        "* $u \\cdot v$ is the dot product (or inner product) of two vectors\n",
        "* $||u||_2$ is the norm (or length) of the vector $u$\n",
        "* $\\theta$ is the angle between $u$ and $v$.\n",
        "* The cosine similarity depends on the angle between $u$ and $v$.\n",
        "    * If $u$ and $v$ are very similar, their cosine similarity will be close to 1.\n",
        "    * If they are dissimilar, the cosine similarity will take a smaller value.\n",
        "\n",
        "<img src=\"images/cosine_sim.png\" style=\"width:800px;height:250px;\">\n",
        "<caption><center><font color='purple'><b>Figure 1</b>: The cosine of the angle between two vectors is a measure of their similarity.</font></center></caption>\n",
        "\n",
        "Implement the function `cosine_similarity()` to evaluate the similarity between word vectors.\n",
        "\n",
        "**Reminder**: The norm of $u$ is defined as $ ||u||_2 = \\sqrt{\\sum_{i=1}^{n} u_i^2}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "sXAv7ldhRDLK"
      },
      "outputs": [],
      "source": [
        "def cosine_similarity(u, v):\n",
        "    \"\"\"\n",
        "    Cosine similarity reflects the degree of similarity between u and v\n",
        "\n",
        "    Arguments:\n",
        "        u -- a word vector of shape (n,)\n",
        "        v -- a word vector of shape (n,)\n",
        "\n",
        "    Returns:\n",
        "        cosine_similarity -- the cosine similarity between u and v defined by the formula above.\n",
        "    \"\"\"\n",
        "\n",
        "    dot_product = np.dot(u, v) # Calculate the dot product of vectors u and v.\n",
        "    norm_u = np.linalg.norm(u) # Calculate the Euclidean norm of vector u.\n",
        "    norm_v = np.linalg.norm(v) # Calculate the Euclidean norm of vector v.\n",
        "    cosine_similarity = dot_product / (norm_u * norm_v) # Calculate the cosine similarity between vectors u and v.\n",
        "\n",
        "    return cosine_similarity\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKL2BLMxRDLL"
      },
      "source": [
        "## find k nearest neighbors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "rBetJpATRDLL"
      },
      "outputs": [],
      "source": [
        "def find_k_nearest_neighbors(word, embedding_dict, k):\n",
        "    \"\"\"\n",
        "    Implement a function to return the nearest words to a specific word based on the given dictionary.\n",
        "\n",
        "    Arguments:\n",
        "        word           -- a word, string\n",
        "        embedding_dict -- dictionary that maps words to their corresponding vectors\n",
        "        k              -- the number of words that should be returned\n",
        "\n",
        "    Returns:\n",
        "        A list of size k consisting of the k most similar words to the given word.\n",
        "\n",
        "    Note: Use the cosine_similarity function to calculate the similarity between words.\n",
        "    \"\"\"\n",
        "    # Get the word vector for the given word\n",
        "    word_vector = embedding_dict.get(word)  # Retrieve the word vector corresponding to the input word from the embedding dictionary\n",
        "\n",
        "    if word_vector is None:\n",
        "        return []  # Return an empty list if the word is not found in the dictionary\n",
        "\n",
        "    # Calculate cosine similarity between the word vector and all other word vectors\n",
        "    similarities = {}  # Initialize an empty dictionary to store similarities\n",
        "    for w, v in embedding_dict.items():  # Iterate over all words and their corresponding vectors in the embedding dictionary\n",
        "        if w != word:  # Exclude the input word itself\n",
        "            similarity = cosine_similarity(word_vector, v)  # Calculate cosine similarity between the input word vector and the current word vector\n",
        "            similarities[w] = similarity  # Store the similarity score in the dictionary\n",
        "\n",
        "    # Sort the dictionary by values (cosine similarities) in descending order\n",
        "    sorted_similarities = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Get the k nearest neighbors\n",
        "    nearest_neighbors = [item[0] for item in sorted_similarities[:k]]  # Extract the words (neighbors) with the highest similarity scores\n",
        "\n",
        "    return nearest_neighbors  # Return the list of k nearest neighbors\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nqr5DLDYuKd-"
      },
      "source": [
        "# 2. One hot encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPqc0I0yuNlI",
        "outputId": "60e2666d-5fc4-4448-bce1-435abaad9850"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Ø§Ø³Ø§ØªÛŒØ¯ÛŒ\n",
            "2. Ø´Ø¯Ù…ğŸ˜­ğŸ˜­ğŸ˜­\n",
            "3. Â«Ù‡Ù†ÙˆØ²Â»\n",
            "4. Ø¢Ù†Ø¬Ø§Ø³Øª\n",
            "5. Ù†Ù…ÛŒØ®ÙˆØ§Ø³ØªÙ…\n",
            "6. Ø±ÛŒØ¯Ù…\n",
            "7. Ù‚Ø§ØªÙ„Ù‡Ø§\n",
            "8. ÛŒØ§Ø°ØªÙˆÙ†Ù‡\n",
            "9. Ø´Ø®ØµÛŒÙ‡\n",
            "10. ÙˆÚ©Øª\n"
          ]
        }
      ],
      "source": [
        "    \"\"\"\n",
        "      1-Vocabulary Creation: It first concatenates all the texts into one string, then splits it into words and creates a list of unique vocabulary words.\n",
        "      2-Converting Vocabulary to One-Hot Encoding Vectors: It uses the OneHotEncoder from the scikit-learn library to convert the vocabulary into one-hot encoding vectors. Here, handle_unknown='ignore' is used to handle unknown inputs silently, and sparse=False is used to produce dense vectors.\n",
        "      3-Creating an Embedding Dictionary: It stores the one-hot encoding vectors corresponding to each word in a dictionary.\n",
        "      4-Returning the Embedding Dictionary: Finally, it returns the dictionary containing words as keys and their corresponding one-hot encoding vectors as values.\n",
        "    \"\"\"\n",
        "    # Create vocabulary based on words present in the texts\n",
        "    vocab = list(set((\" \".join(texts)).split()))\n",
        "    # Convert vocabulary to an array and reshape it to be column-wise\n",
        "    vocab = np.array(vocab).reshape(-1, 1)\n",
        "    # Create an instance of the OneHotEncoder with specified behavior for encountering unknown tokens\n",
        "    encoding = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "    # Transform vocabulary into one-hot vectors using the encoder\n",
        "    one_hot_encoding = encoding.fit_transform(vocab)\n",
        "    # Create a dictionary for mapping words to their corresponding one-hot vectors\n",
        "    embedding_dict = {vocab[i][0]: one_hot_encoding[i] for i in range(len(vocab))}\n",
        "    # Set the number of nearest neighbors to find\n",
        "    k = 10\n",
        "    # Set the word for which to find the nearest neighbors\n",
        "    word = \"Ø¢Ø²Ø§Ø¯ÛŒ\"\n",
        "    # Call the function find_k_nearest_neighbors to find the k nearest neighbors of the given word in the embedding dictionary\n",
        "    # The function returns a list of the k nearest words\n",
        "    k_nearest_words = find_k_nearest_neighbors(word, embedding_dict, k)\n",
        "\n",
        "    # Print the list of k nearest words with their indices\n",
        "    for i, nearest_word in enumerate(k_nearest_words, start=1):\n",
        "        print(f\"{i}. {nearest_word}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hraxjUTiRDLL"
      },
      "source": [
        "##### Describe advantages and disadvantages of one-hot encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9tAFn4brKrB"
      },
      "source": [
        "One-hot encoding is a technique used in machine learning and data processing to represent categorical variables as binary vectors. Each category is represented by a vector where only one element is 1 and the rest are 0s. Here are some advantages and disadvantages of using one-hot encoding:</n><br><br>\n",
        "\n",
        "\n",
        "Advantage:\n",
        "<br><br>\n",
        "\n",
        "1- Preservation of Information: One-hot encoding preserves the categorical nature of variables without imposing ordinality or numerical relationships that may not exist.\n",
        "\n",
        "2- Compatibility with Algorithms: Many machine learning algorithms and models require numerical input. One-hot encoding transforms categorical data into a format that these algorithms can process.\n",
        "\n",
        "3- Avoidance of Numerical Bias: By using binary values, one-hot encoding prevents the introduction of artificial numerical relationships that could bias the model.\n",
        "\n",
        "4- Interpretability: One-hot encoding results in interpretable features. Each feature represents a single category, making it clear which category is being referred to.\n",
        "\n",
        "5- Generalization: One-hot encoding can handle categorical variables with any number of unique categories, making it suitable for a wide range of datasets.<br><br>\n",
        "\n",
        "\n",
        "Disadvantage:\n",
        "<br><br>\n",
        "\n",
        "1- Dimensionality Increase: One-hot encoding can significantly increase the dimensionality of the dataset, especially when dealing with categorical variables with many unique categories. This can lead to the curse of dimensionality and increased computational complexity.\n",
        "\n",
        "2- Sparse Representation: The resulting one-hot encoded vectors are sparse, consisting mostly of 0s. This can consume a lot of memory and storage space, especially for datasets with a large number of unique categories.\n",
        "\n",
        "3- Loss of Information about Relationships: One-hot encoding treats each category as independent, disregarding any potential relationships or similarities between categories. This may lead to loss of information, especially in cases where there is inherent ordinality or hierarchical structure among categories.\n",
        "\n",
        "4- Difficulty Handling New Categories: If new categories are encountered during model deployment that were not present in the training data, it may be challenging to handle them appropriately without retraining the model or using additional techniques like hashing or embedding.\n",
        "\n",
        "5- Potential for Overfitting: In models with limited data, one-hot encoding can lead to overfitting, especially if there are many rare categories. Each category gets its own dimension, which the model may try to fit even if it's not statistically significant.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHeSYFUKw5gw"
      },
      "source": [
        "# 3. TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJMju0Tiw9YA"
      },
      "outputs": [],
      "source": [
        "# 1. find the TF-IDF of all tweets.\n",
        "# 2. choose one tweets randomly.\n",
        "# 3. find 10 nearest tweets from chosen tweet.\n",
        "\n",
        "\n",
        "# Function to count occurrences of a word in a sentence\n",
        "def count_word_occurrences(word, sentence):\n",
        "    return (sentence.split()).count(word)\n",
        "\n",
        "# Function to calculate the term frequency (TF) matrix\n",
        "def calculate_tf_matrix(texts_list, vocab_list):\n",
        "    # Creating a mapping of text indices to their respective positions in the texts_list\n",
        "    text_to_index = {texts_list[i]: i for i in range(len(texts_list))}\n",
        "    # Creating a reverse mapping of text indices\n",
        "    index_to_text = {i: texts_list[i] for i in range(len(texts_list))}\n",
        "    # Assertion to check if the mappings have consistent lengths\n",
        "    assert len(text_to_index) == len(index_to_text), \"Mismatch in lengths of text_to_index and index_to_text\"\n",
        "\n",
        "    # Creating a mapping of vocabulary words to their respective positions\n",
        "    word_to_index = {vocab_list[i][0]: i for i in range(len(vocab_list))}\n",
        "    # Creating a reverse mapping of word indices\n",
        "    index_to_word = {i: vocab_list[i][0] for i in range(len(vocab_list))}\n",
        "    # Assertion to check if the mappings have consistent lengths\n",
        "    assert len(word_to_index) == len(index_to_word), \"Mismatch in lengths of word_to_index and index_to_word\"\n",
        "\n",
        "    # Getting the number of words in the vocabulary\n",
        "    num_words = len(word_to_index)\n",
        "    # Getting the number of documents (texts) in the texts_list\n",
        "    num_documents = len(text_to_index)\n",
        "\n",
        "    # Initializing a TF matrix with zeros, shaped according to the vocabulary size and number of documents\n",
        "    tf_matrix = np.zeros(shape=(num_words, num_documents))\n",
        "\n",
        "    # Looping through each word in the vocabulary\n",
        "    for word_index in tqdm(range(num_words)):\n",
        "        # Looping through each document (text)\n",
        "        for document_index in range(num_documents):\n",
        "            # Counting the occurrences of the word in the current document\n",
        "            word_count = count_word_occurrences(index_to_word[word_index], index_to_text[document_index])\n",
        "            # Calculating the term frequency using the formula: (1 + log(word_count)) if word_count > 0, else 0\n",
        "            tf_matrix[word_index, document_index] = (1 + np.log(word_count)) if word_count > 0 else 0\n",
        "\n",
        "    # Returning the calculated TF matrix\n",
        "    return tf_matrix, index_to_word, num_documents\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymkOnMwyJpmV",
        "outputId": "1319c32f-b2b8-4ce5-e993-62a520ef9855"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25916/25916 [18:33<00:00, 23.27it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25916/25916 [00:08<00:00, 3165.69it/s]\n"
          ]
        }
      ],
      "source": [
        "texts_list = texts\n",
        "# Assign the variable texts_list to the texts list.\n",
        "\n",
        "# Create vocabulary based on words present in the texts\n",
        "vocab_list = list(set((\" \".join(texts_list)).split()))\n",
        "# Create a text by joining all texts, split it into words, get unique words, and convert them into a list.\n",
        "# The variable vocab holds a list of unique words present in the texts.\n",
        "\n",
        "# Convert vocabulary to an array and reshape it to be column-wise\n",
        "vocab_list = np.array(vocab_list).reshape(-1, 1)\n",
        "# Convert the list vocab into an array and reshape it into a column-wise shape.\n",
        "# The variable vocab holds an array of unique words in a column-wise format.\n",
        "\n",
        "\n",
        "# Calculate term frequency (TF) matrix, index_to_word, and the number of documents\n",
        "tf_matrix, index_to_word, num_documents = calculate_tf_matrix(texts_list, vocab_list)\n",
        "\n",
        "# Initialize a dictionary for storing TF-IDF values\n",
        "tf_idf = {}\n",
        "\n",
        "# Loop over each word\n",
        "for i in tqdm(range(len(index_to_word))):\n",
        "    # Calculate document frequency (DF) for the current word\n",
        "    df = np.sum(tf_matrix[i])\n",
        "    # Compute inverse document frequency (IDF)\n",
        "    temp = np.log(num_documents/df)\n",
        "    # Calculate TF-IDF for the current word and store it in the dictionary\n",
        "    tf_idf[index_to_word[i]] = tf_matrix[i] * temp\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ekGpSFqRDLM"
      },
      "source": [
        "##### Describe advantages and disadvantages of TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVwayXJnjVXV",
        "outputId": "0f3232f7-6308-4bd9-904d-23ffcfba4534"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1. Ú¯ÙˆÙ‡ Ø¨Ù‡ Ù‚Ø¨Ø± Ù¾Ø¯Ø±Øª Ø®Ø§Ù…Ù†Ù‡ Ø§ÛŒ\n",
            "2. Ø§ÛŒ Ø¨Ù‡ Ú†Ø´Ù…\n",
            "3. Ù…Ø±Ú¯ Ø¨Ù‡ Ù†ÛŒØ±Ù†Ú¯Ø´ÙˆÙ†\n",
            "4. Ø¯Ø±ÛŒÚ†Ù‡ Ø§ÛŒ Ø¨Ù‡ Ø¢Ø²Ø§Ø¯ÛŒ\n",
            "5. Ø¨Ù‡ Ø§Ù…ÛŒØ¯\n",
            "6. Ø¨Ù‡ Ø§Ù…ÛŒØ¯ Ù¾ÛŒØ±ÙˆØ²ÛŒğŸ¤\n",
            "7. Ø¨Ù‡ Ø§Ù…ÛŒØ¯ Ø¢Ø²Ø§Ø¯ÛŒâ¦ğŸ•Šï¸â©\n",
            "8. Ø¨Ù‡ Ø§Ù…ÛŒØ¯ Ø¢Ø²Ø§Ø¯ÛŒğŸ¤ğŸ»\n",
            "9. Ø¨Ù‡ Ø§Ù…ÛŒØ¯ Û²Û°ØªØ§ÛŒÛŒÛŒ\n",
            "10. Ø¨Ù‡ Ø§Ù…ÛŒØ¯ Ø§Ø²Ø§Ø¯ÛŒğŸ”¥\n"
          ]
        }
      ],
      "source": [
        "def encode_tweet(text):\n",
        "  # Split the input text into individual words\n",
        "  words = text.split()\n",
        "\n",
        "  # Calculate the TF-IDF value for each word in the text using a pre-calculated tf_idf dictionary\n",
        "  words_vec = [tf_idf[w] for w in words]\n",
        "\n",
        "  # Calculate the average TF-IDF value for all words in the text\n",
        "  # by summing up the TF-IDF values of all words and dividing by the total number of words\n",
        "  return np.array(sum(words_vec)) / len(words)\n",
        "\n",
        "# Create a dictionary called embedding_dict where keys are the texts and values are their corresponding encoded representations\n",
        "# by applying the encode_tweet function to each text in the texts list\n",
        "embedding_dict = {t: encode_tweet(t) for t in texts}\n",
        "\n",
        "# Select a random tweet from the list of texts\n",
        "tweet = texts[np.random.randint(0, len(texts))]\n",
        "\n",
        "# Define the number of nearest neighbors to find\n",
        "k = 10\n",
        "\n",
        "# Find the k nearest neighbors of the selected tweet based on their embeddings\n",
        "k_nearest_tweets = find_k_nearest_neighbors(tweet, embedding_dict, k)\n",
        "\n",
        "# Print the list of k nearest neighbor tweets in a columnar format with row numbers\n",
        "for i, tweet in enumerate(k_nearest_tweets, start=1):\n",
        "    print(f\"{i}. {tweet}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GHvpTAZu7ZU"
      },
      "source": [
        "TF-IDF (Term Frequency-Inverse Document Frequency) is a statistical measure used to evaluate the importance of a word in a document relative to a collection of documents. Here are some advantages and disadvantages of using TF-IDF:<br><br>\n",
        "\n",
        "Advatages:<br><br>\n",
        "1- Weighted term importance: TF-IDF assigns weights to terms based on their frequency in the document and their rarity across the corpus. This helps in identifying important terms within a document.\n",
        "\n",
        "2- Reduces the impact of common terms: TF-IDF reduces the importance of terms that occur frequently across all documents in the corpus. Common words like \"the\", \"is\", etc., which may not carry much semantic meaning, are often penalized in their importance.\n",
        "\n",
        "3- Suitable for large datasets: TF-IDF works well even with large datasets since it doesn't rely on the entire vocabulary of the corpus but rather on the terms present in individual documents.\n",
        "\n",
        "4- Handles sparse data well: In datasets where most words occur only a few times, TF-IDF can effectively handle the sparsity by focusing on the specific terms that are present.\n",
        "\n",
        "5- Simple and efficient: Implementation of TF-IDF is relatively straightforward, making it easy to understand and implement in various applications.\n",
        "<br><br>\n",
        "\n",
        "Disadvantages:\n",
        "<br><br>\n",
        "1- Ignores word order and context: TF-IDF treats documents as bags of words, disregarding the word order and context in which the words appear. This can lead to loss of information, especially in tasks where word order is crucial, such as in natural language processing tasks like sentiment analysis or text summarization.\n",
        "\n",
        "2- Doesn't consider semantics: TF-IDF only considers the frequency of terms in documents and doesn't take into account the meaning or semantics of words. Therefore, it may not always capture the true relevance of a term to a document's content.\n",
        "\n",
        "3- Sensitive to document length: Longer documents may have higher overall term frequencies compared to shorter documents, which can skew the TF-IDF scores. Normalization techniques can mitigate this, but it remains a consideration.\n",
        "\n",
        "4- Requires a large corpus: TF-IDF relies on a corpus of documents to calculate inverse document frequency. In cases where the corpus is small or not representative of the domain, TF-IDF may not perform optimally.\n",
        "\n",
        "5- Not effective for some tasks: In tasks such as sentiment analysis or document classification where the focus is on understanding the overall context or sentiment of the document, TF-IDF alone may not be sufficient and more advanced techniques like word embeddings or deep learning models may be needed.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INM6vtm2zqJs"
      },
      "source": [
        "# 4. Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCnxqaVY2zCc",
        "outputId": "9e0d76cb-90b1-448a-ec69-ac32f2fd7b64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1. Ø§Ø²Ø§Ø¯ÛŒ\n",
            "2. ÙØ±Ø²Ù†Ø¯Ø§Ù†\n",
            "3. Ø¢Ø²Ø§Ø¯ÛŒÙ…ÙˆÙ†\n",
            "4. Ù‚ÛŒØ§Ù…\n",
            "5. Ø¢Ø²Ø§Ø¯ÛŒÂ»\n",
            "6. ÙˆØ·Ù†Ù…Ø§Ù†\n",
            "7. Ø´Ø§Ø¯ÛŒ\n",
            "8. Ø¢ØºØ§Ø²\n",
            "9. Ù¾ÛŒØ±ÙˆØ²ÛŒ\n",
            "10. Ø®ÛŒØ§Ø¨Ø§Ù†\n"
          ]
        }
      ],
      "source": [
        "# 1. train a word2vec model base on all tweets\n",
        "# 2. find 10 nearest words from \"Ø¢Ø²Ø§Ø¯ÛŒ\"\n",
        "# Tokenize each text in the 'texts' list by splitting them into individual words\n",
        "tokenized_texts = [t.split() for t in texts]\n",
        "\n",
        "# Train a Word2Vec model on the tokenized texts with 100 epochs\n",
        "model = Word2Vec(tokenized_texts, epochs=100)\n",
        "\n",
        "# Retrieve the vocabulary of the trained Word2Vec model\n",
        "vocab = model.wv.key_to_index\n",
        "\n",
        "# Create an embedding dictionary where keys are words from the vocabulary and values are their corresponding embeddings\n",
        "embedding_dict = {word: model.wv[word] for word in vocab}\n",
        "\n",
        "word = \"Ø¢Ø²Ø§Ø¯ÛŒ\"\n",
        "# Define the number of nearest neighbors to find\n",
        "k = 10\n",
        "# Find the k nearest neighbors of the selected tweet based on their embeddings\n",
        "k_nearest_words = find_k_nearest_neighbors(word, embedding_dict, k)\n",
        "# Print the list of k nearest neighbor words in a columnar format with row numbers\n",
        "for i, word in enumerate(k_nearest_words, start=1):\n",
        "    print(f\"{i}. {word}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kry2kCIBRDLM"
      },
      "source": [
        "##### Describe advantages and disadvantages of Word2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xv39LXf0wkjd"
      },
      "source": [
        "Word2Vec is a popular technique in natural language processing (NLP) used to generate distributed representations of words in a continuous vector space. Here are some advantages and disadvantages of Word2Vec:<br><br>\n",
        "\n",
        "Advantages:<br><br>\n",
        "1- Semantic Similarity: Word2Vec captures semantic similarities between words by representing them as vectors in a continuous space. This means that words with similar meanings are often closer together in the vector space, allowing for more nuanced understanding of language.\n",
        "\n",
        "2- Efficiency: Word2Vec models are computationally efficient compared to other methods for generating word embeddings, such as co-occurrence matrices or neural network-based approaches like GloVe. This efficiency makes Word2Vec suitable for training on large text corpora.\n",
        "\n",
        "3- Dimensionality Reduction: Word2Vec effectively reduces the dimensionality of the word space while preserving semantic relationships. This allows for more efficient storage and processing of word embeddings, making them easier to work with in downstream NLP tasks.\n",
        "\n",
        "4- Pre-trained Models: Pre-trained Word2Vec models trained on large text corpora are readily available, which saves time and computational resources for developers who can leverage these pre-trained embeddings for their specific tasks instead of training from scratch.\n",
        "\n",
        "5- Transfer Learning: Word2Vec embeddings can be transferred and fine-tuned for downstream NLP tasks such as text classification, sentiment analysis, and machine translation. This transfer learning capability enables models to benefit from the semantic knowledge captured during Word2Vec training.\n",
        "<br><br>\n",
        "\n",
        "Disadvantages:<br><br>\n",
        "1- Contextual Information: Word2Vec does not capture contextual information, meaning that the same word may have different representations depending on its context. This limitation can lead to suboptimal performance in tasks where context is crucial, such as disambiguation or language generation.\n",
        "\n",
        "2- Out-of-vocabulary Words: Word2Vec struggles with out-of-vocabulary words, as it can only generate embeddings for words seen during training. Rare or unseen words may not have meaningful representations in the Word2Vec space, which can affect the performance of downstream tasks, especially in domains with specialized terminology.\n",
        "\n",
        "3- Polysemy and Homonymy: Word2Vec may struggle to disambiguate words with multiple meanings (polysemy) or words that are spelled the same but have different meanings (homonymy). In such cases, the word embeddings may not accurately capture the intended semantics, leading to errors in downstream tasks.\n",
        "\n",
        "4- Training Data Bias: Word2Vec embeddings are trained on large text corpora, which may contain biases present in the data, such as cultural or gender biases. These biases can be inadvertently propagated to downstream applications, potentially amplifying societal prejudices or stereotypes.\n",
        "\n",
        "5- Fixed Embedding Size: Word2Vec generates fixed-size embeddings for words, meaning that all words are represented by vectors of the same length. This fixed-size representation may not capture the full complexity of language, especially for words with rich semantic or syntactic properties.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSdlWMl64aPN"
      },
      "source": [
        "# 5. Contextualized embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgVAEQhyOPxg",
        "outputId": "402dd931-e98b-464a-eb20-3e17ce8791a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers[sentencepiece] in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (3.13.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (4.66.2)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.1.99)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (3.20.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers[sentencepiece]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers[sentencepiece]) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (2024.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnAtKMx3taA-",
        "outputId": "1dfc6dfc-eaf2-42fc-ace2-43bebc2b8216"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting accelerate\n",
            "  Downloading accelerate-0.28.0-py3-none-any.whl (290 kB)\n",
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/290.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”\u001b[0m \u001b[32m245.8/290.1 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m290.1/290.1 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.2.1+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m87.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m80.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, accelerate\n",
            "Successfully installed accelerate-0.28.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ],
      "source": [
        "!pip install accelerate -U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289,
          "referenced_widgets": [
            "6a1a31889dca45469a837b2d35fdf5b4",
            "cdea9767d3224cda9c2937842e50a869",
            "cacca3c9a6a14fe4b84f42c76f4dc47d",
            "e2ed037e79d846989ae4873096f704bb",
            "1bf1dce8061c40ea8e358414d481fcab",
            "2f178031a452422a893dab3a9f6db629",
            "40636d9c49c44e5b9c76abb8fbad0723",
            "02ec28d233f54084b0e7f975886c75bc",
            "b5ba9124d7454ede8ee4ea6b9ef8b94a",
            "58bdb175328646bdb406d1a77b273977",
            "b60d0904dcfd40b2bf09860355462124",
            "a4d6cd436da84e10989db21ae53527a3",
            "6d0e5c799be04439a09fbf72c4b317f8",
            "34ffc79da4154500b678a68a89a5ac69",
            "1ea74e772e90406bab34764d09e78bf9",
            "19690aeb30ed414b9741b15986260b34",
            "1259ee92aba946249787faf9ab282e84",
            "e172d422300e4be5ac1180f190e9c1a1",
            "f8af127c3d2e41b3bb8cbbdc51ca18a2",
            "18a09f07bf1a4cb1bb5bb863d7f31545",
            "0aec0b6809f944f2ac1d24768552a4be",
            "3d5203a5244b48478acb8c30c6712d21",
            "3c36968e7f714757b6277d7b0455720c",
            "033ea22439e34c26b740143731956f34",
            "5f6bb13d8a4043f58c159f4fedbc06c6",
            "8eb5d6cde2ba40c09ece15923dd42435",
            "9614e0aa84bc484187ddd07b04d6b6c3",
            "b9f1aff7747246979c28331839aaf64f",
            "eb8e2951c0bd4474b99a6336f805904c",
            "b4a40d67dc574aa6a8c66920eeeae8ce",
            "40641406cff846589e9278e8656e2c75",
            "e640ef8472ee4d7aad5defdcdf96fa54",
            "db98c90c9ea34e28baa954f36b5df6e7"
          ]
        },
        "id": "GfKEqNml6eEB",
        "outputId": "6e4b6bbf-6175-414f-cbd4-02b95d58f3dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/1.22M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6a1a31889dca45469a837b2d35fdf5b4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/434 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a4d6cd436da84e10989db21ae53527a3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/654M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3c36968e7f714757b6277d7b0455720c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at HooshvareLab/bert-base-parsbert-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "# Load model and tokenizer\n",
        "\n",
        "# Import necessary modules\n",
        "from transformers import BertForMaskedLM, BertTokenizer, AdamW\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Specify the pre-trained model name\n",
        "model_name = \"HooshvareLab/bert-base-parsbert-uncased\"\n",
        "\n",
        "# Define the device for computation (GPU if available, otherwise CPU)\n",
        "device = torch.device('cuda' if torch.cuda.is_available else 'cpu')\n",
        "\n",
        "# Initialize the tokenizer using the specified pre-trained model\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Initialize the model for Masked Language Modeling (MLM) using the specified pre-trained model\n",
        "# Move the model to the specified device (GPU or CPU)\n",
        "model = BertForMaskedLM.from_pretrained(model_name).to(device)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cyx4iwcNxx-4",
        "outputId": "f4a7efd2-c648-4c8c-978d-9aee81d4707a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [09:56<00:00,  1.15s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0- loss: 44.535070070720394\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [09:58<00:00,  1.15s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1- loss: 0.15834053394792136\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [09:58<00:00,  1.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2- loss: 0.06375286062029772\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Define a custom Dataset class for your text data\n",
        "class TextDataset(Dataset):\n",
        "    # The initializer takes a list of texts and a tokenizer\n",
        "    def __init__(self, texts, tokenizer):\n",
        "        self.tokenizer = tokenizer  # Store the tokenizer\n",
        "        # Tokenize the texts and store the result\n",
        "        self.inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True)\n",
        "\n",
        "    # The __len__ method returns the number of texts\n",
        "    def __len__(self):\n",
        "        return len(self.inputs[\"input_ids\"])\n",
        "\n",
        "    # The __getitem__ method returns the inputs for the text at the given index\n",
        "    def __getitem__(self, idx):\n",
        "        return {key: val[idx] for key, val in self.inputs.items()}\n",
        "\n",
        "# Create an instance of your custom Dataset\n",
        "dataset = TextDataset(texts, tokenizer)\n",
        "\n",
        "# Create a DataLoader to handle batching of your data\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Initialize the AdamW optimizer with a learning rate of 1e-4\n",
        "optimizer = AdamW(model.parameters(), lr=1e-4)\n",
        "\n",
        "# Put the model in training mode\n",
        "model.train()\n",
        "\n",
        "# Train the model for 3 epochs\n",
        "for epoch in range(3):\n",
        "    epoch_loss = 0  # Initialize the loss for this epoch\n",
        "    # Loop over each batch in the DataLoader\n",
        "    for batch in tqdm(dataloader, total=len(dataloader)):\n",
        "        optimizer.zero_grad()  # Zero the gradients\n",
        "        # Move the batch data to the device\n",
        "        inputs = {key: val.to(device) for key, val in batch.items()}\n",
        "        # Use the input ids as labels\n",
        "        inputs['labels'] = inputs['input_ids'].detach().clone()\n",
        "        # Forward pass: compute the model outputs\n",
        "        outputs = model(**inputs)\n",
        "        # Compute the loss\n",
        "        loss = outputs.loss\n",
        "        # Accumulate the loss over the epoch\n",
        "        epoch_loss += loss.item()\n",
        "        # Backward pass: compute the gradients\n",
        "        loss.backward()\n",
        "        # Update the model parameters\n",
        "        optimizer.step()\n",
        "    # Print the loss for this epoch\n",
        "    print(f'{epoch}- loss: {epoch_loss}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "uc8hBCnn4cV_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "167e8ac1-9e99-48b2-dd0a-4e0581ee6d2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Ø§Ø²Ø§Ø¯Ø³Ø§Ø²ÛŒ\n",
            "2. Ø§Ø²Ø§Ø¯\n",
            "3. Ø§Ø²Ø§Ø¯ÛŒÙ‡Ø§ÛŒ\n",
            "4. Ø§Ø²Ø§Ø¯Ù‰\n",
            "5. Ø§Ø²Ø§Ø¯ÛŒÙ‡Ø§\n",
            "6. Ø±Ù‡Ø§ÛŒÛŒ\n",
            "7. Ø§Ø²Ø§Ø¯Ø§Ù†Ù‡\n",
            "8. Ø§Ø³Ø§ÛŒØ´\n",
            "9. Ø§Ø²Ø§Ø¯Ø´Ø¯Ù†\n",
            "10. Ø§Ù†Ø¹Ø·Ø§ÙÙ¾Ø°ÛŒØ±ÛŒ\n"
          ]
        }
      ],
      "source": [
        "# 1. fine-tune the model base on all tweets\n",
        "# 2. find 10 nearest words from \"Ø¢Ø²Ø§Ø¯ÛŒ\"\n",
        "\n",
        "word_embeddings = model.bert.embeddings.word_embeddings  # Extracting word embeddings layer from BERT model.\n",
        "\n",
        "# Creating a dictionary where keys are words and values are their corresponding embeddings.\n",
        "embedding_dict = {\n",
        "    word: word_embeddings(torch.tensor([tokenizer.convert_tokens_to_ids(word)])  # Convert word to token ID, then tensor.\n",
        "                          .to(device))  # Move tensor to appropriate device (CPU/GPU).\n",
        "                          .cpu()  # Move tensor back to CPU for detach operation.\n",
        "                          .squeeze(0)  # Remove the batch dimension.\n",
        "                          .detach()  # Detach tensor from computation graph.\n",
        "                          .numpy()  # Convert tensor to NumPy array.\n",
        "    for word in tokenizer.get_vocab().keys()  # Iterate over all words in tokenizer's vocabulary.\n",
        "}\n",
        "\n",
        "\n",
        "word = \"Ø§Ø²Ø§Ø¯ÛŒ\"\n",
        "# Define the number of nearest neighbors to find\n",
        "k = 10\n",
        "\n",
        "# The word of interest for finding its nearest neighbors is specified here.\n",
        "\n",
        "k_nearest_words = find_k_nearest_neighbors(word, embedding_dict, k)\n",
        "\n",
        "# The function find_k_nearest_neighbors finds the closest similar words to a given word from the embedding_dict dataset and stores them in k_nearest_words.\n",
        "# These functions perform computational operations related to embedding vectors.\n",
        "\n",
        "# Print the list of k nearest words with their indices\n",
        "for i, nearest_word in enumerate(k_nearest_words, start=1):\n",
        "    print(f\"{i}. {nearest_word}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vor7_eDyRDLN"
      },
      "source": [
        "##### Describe advantages and disadvantages of Contextualized embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Al-4HT8KxTwd"
      },
      "source": [
        "Contextualized word embeddings, such as those produced by models like ELMo, GPT, and BERT, offer several advantages and disadvantages:<br><br>\n",
        "\n",
        "Advantages:\n",
        "<br><br>\n",
        "1- Contextual Understanding: Contextualized embeddings capture the meaning of a word based on its context within a sentence. This allows for a deeper understanding of the word's semantics compared to static embeddings like Word2Vec or GloVe.\n",
        "\n",
        "2- Polysemy Handling: Words can have multiple meanings (polysemy), and contextual embeddings can capture these nuances better by considering the surrounding context. For instance, \"bank\" could refer to a financial institution or the side of a river.\n",
        "\n",
        "3- Transfer Learning: Pre-trained contextualized embeddings can be fine-tuned on specific downstream tasks with relatively small amounts of task-specific data. This transfer learning approach often leads to improved performance on various NLP tasks.\n",
        "\n",
        "4- Dynamic Representations: Unlike static embeddings, which assign a fixed representation to each word, contextualized embeddings produce dynamic representations that vary based on the context. This dynamic nature helps in capturing subtle changes in meaning across different contexts.\n",
        "\n",
        "5- Out-of-Vocabulary (OOV) Handling: Contextualized embeddings can generate representations for out-of-vocabulary words based on their context. This is particularly useful for handling rare or domain-specific terms that may not be present in the training vocabulary.\n",
        "<br><br>\n",
        "\n",
        "\n",
        "Disadvantages:\n",
        "<br><br>\n",
        "1- Computational Complexity: Contextualized embedding models are computationally intensive and require significant resources for training and inference compared to static embedding models. Fine-tuning on downstream tasks can also be time-consuming.\n",
        "\n",
        "2- Limited Interpretability: The contextualized embeddings produced by models like BERT or GPT are high-dimensional and lack direct interpretability. Understanding why a particular representation is generated for a word in a given context can be challenging.\n",
        "\n",
        "3- Data Dependency: Contextualized embeddings heavily rely on large amounts of annotated text data for pre-training. This dependency on data availability can be a limitation, especially for low-resource languages or specialized domains with limited text corpora.\n",
        "\n",
        "4- Domain Specificity: Pre-trained contextualized embeddings may not capture domain-specific nuances effectively, especially if the pre-training data is not representative of the target domain. Fine-tuning on domain-specific data can help mitigate this issue to some extent.\n",
        "\n",
        "5- Context Window Limitation: Although contextualized embeddings consider the surrounding context, they are still limited by the window size used during pre-training. Long-range dependencies or context beyond the specified window may not be fully captured, impacting the model's understanding of the text.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6a1a31889dca45469a837b2d35fdf5b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cdea9767d3224cda9c2937842e50a869",
              "IPY_MODEL_cacca3c9a6a14fe4b84f42c76f4dc47d",
              "IPY_MODEL_e2ed037e79d846989ae4873096f704bb"
            ],
            "layout": "IPY_MODEL_1bf1dce8061c40ea8e358414d481fcab"
          }
        },
        "cdea9767d3224cda9c2937842e50a869": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f178031a452422a893dab3a9f6db629",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_40636d9c49c44e5b9c76abb8fbad0723",
            "value": "vocab.txt:â€‡100%"
          }
        },
        "cacca3c9a6a14fe4b84f42c76f4dc47d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_02ec28d233f54084b0e7f975886c75bc",
            "max": 1215509,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b5ba9124d7454ede8ee4ea6b9ef8b94a",
            "value": 1215509
          }
        },
        "e2ed037e79d846989ae4873096f704bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_58bdb175328646bdb406d1a77b273977",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_b60d0904dcfd40b2bf09860355462124",
            "value": "â€‡1.22M/1.22Mâ€‡[00:00&lt;00:00,â€‡16.7MB/s]"
          }
        },
        "1bf1dce8061c40ea8e358414d481fcab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f178031a452422a893dab3a9f6db629": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40636d9c49c44e5b9c76abb8fbad0723": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "02ec28d233f54084b0e7f975886c75bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5ba9124d7454ede8ee4ea6b9ef8b94a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "58bdb175328646bdb406d1a77b273977": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b60d0904dcfd40b2bf09860355462124": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a4d6cd436da84e10989db21ae53527a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6d0e5c799be04439a09fbf72c4b317f8",
              "IPY_MODEL_34ffc79da4154500b678a68a89a5ac69",
              "IPY_MODEL_1ea74e772e90406bab34764d09e78bf9"
            ],
            "layout": "IPY_MODEL_19690aeb30ed414b9741b15986260b34"
          }
        },
        "6d0e5c799be04439a09fbf72c4b317f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1259ee92aba946249787faf9ab282e84",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_e172d422300e4be5ac1180f190e9c1a1",
            "value": "config.json:â€‡100%"
          }
        },
        "34ffc79da4154500b678a68a89a5ac69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f8af127c3d2e41b3bb8cbbdc51ca18a2",
            "max": 434,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_18a09f07bf1a4cb1bb5bb863d7f31545",
            "value": 434
          }
        },
        "1ea74e772e90406bab34764d09e78bf9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0aec0b6809f944f2ac1d24768552a4be",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_3d5203a5244b48478acb8c30c6712d21",
            "value": "â€‡434/434â€‡[00:00&lt;00:00,â€‡10.4kB/s]"
          }
        },
        "19690aeb30ed414b9741b15986260b34": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1259ee92aba946249787faf9ab282e84": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e172d422300e4be5ac1180f190e9c1a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f8af127c3d2e41b3bb8cbbdc51ca18a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18a09f07bf1a4cb1bb5bb863d7f31545": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0aec0b6809f944f2ac1d24768552a4be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d5203a5244b48478acb8c30c6712d21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3c36968e7f714757b6277d7b0455720c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_033ea22439e34c26b740143731956f34",
              "IPY_MODEL_5f6bb13d8a4043f58c159f4fedbc06c6",
              "IPY_MODEL_8eb5d6cde2ba40c09ece15923dd42435"
            ],
            "layout": "IPY_MODEL_9614e0aa84bc484187ddd07b04d6b6c3"
          }
        },
        "033ea22439e34c26b740143731956f34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b9f1aff7747246979c28331839aaf64f",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_eb8e2951c0bd4474b99a6336f805904c",
            "value": "pytorch_model.bin:â€‡100%"
          }
        },
        "5f6bb13d8a4043f58c159f4fedbc06c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b4a40d67dc574aa6a8c66920eeeae8ce",
            "max": 654186735,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_40641406cff846589e9278e8656e2c75",
            "value": 654186735
          }
        },
        "8eb5d6cde2ba40c09ece15923dd42435": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e640ef8472ee4d7aad5defdcdf96fa54",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_db98c90c9ea34e28baa954f36b5df6e7",
            "value": "â€‡654M/654Mâ€‡[00:07&lt;00:00,â€‡34.8MB/s]"
          }
        },
        "9614e0aa84bc484187ddd07b04d6b6c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9f1aff7747246979c28331839aaf64f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb8e2951c0bd4474b99a6336f805904c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b4a40d67dc574aa6a8c66920eeeae8ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40641406cff846589e9278e8656e2c75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e640ef8472ee4d7aad5defdcdf96fa54": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db98c90c9ea34e28baa954f36b5df6e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}